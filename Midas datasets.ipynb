{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf41afe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Compute depth maps for images in the input folder.\n",
    "\"\"\"\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import utils\n",
    "import cv2\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from imutils.video import VideoStream\n",
    "from midas.model_loader import default_models, load_model\n",
    "\n",
    "first_execution = True\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49815383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00dbaa48ecaf467fae3cb4d56213de3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/728 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration Isamu136--big-animal-dataset-high-res-embedding-ccb1b56dcdb50954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to D:/cache/huggingface/datasets/Isamu136___parquet/Isamu136--big-animal-dataset-high-res-embedding-ccb1b56dcdb50954/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a7db7c3070649649ed1f546964f086a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b1d04ca8ba4e1ab1a5d4ea1d70831f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/549M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00608cb42f58452f85c0bcb74b3689b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/486M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765d90b069a84894ba8f4dc117f701ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/515M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a81035a02e47fd86941aa99111546a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/26180 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to D:/cache/huggingface/datasets/Isamu136___parquet/Isamu136--big-animal-dataset-high-res-embedding-ccb1b56dcdb50954/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6e8152976d4ceb9bf1df5507bf4f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"Isamu136/big-animal-dataset-high-res-embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "596bfff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(device, model, model_type, image, input_size, target_size, optimize, use_camera):\n",
    "    \"\"\"\n",
    "    Run the inference and interpolate.\n",
    "\n",
    "    Args:\n",
    "        device (torch.device): the torch device used\n",
    "        model: the model used for inference\n",
    "        model_type: the type of the model\n",
    "        image: the image fed into the neural network\n",
    "        input_size: the size (width, height) of the neural network input (for OpenVINO)\n",
    "        target_size: the size (width, height) the neural network output is interpolated to\n",
    "        optimize: optimize the model to half-floats on CUDA?\n",
    "        use_camera: is the camera used?\n",
    "\n",
    "    Returns:\n",
    "        the prediction\n",
    "    \"\"\"\n",
    "    global first_execution\n",
    "\n",
    "    if \"openvino\" in model_type:\n",
    "        if first_execution or not use_camera:\n",
    "            print(f\"    Input resized to {input_size[0]}x{input_size[1]} before entering the encoder\")\n",
    "            first_execution = False\n",
    "\n",
    "        sample = [np.reshape(image, (1, 3, *input_size))]\n",
    "        prediction = model(sample)[model.output(0)][0]\n",
    "        prediction = cv2.resize(prediction, dsize=target_size,\n",
    "                                interpolation=cv2.INTER_CUBIC)\n",
    "    else:\n",
    "        sample = torch.from_numpy(image).to(device).unsqueeze(0)\n",
    "\n",
    "        if optimize and device == torch.device(\"cuda\"):\n",
    "            if first_execution:\n",
    "                print(\"  Optimization to half-floats activated. Use with caution, because models like Swin require\\n\"\n",
    "                      \"  float precision to work properly and may yield non-finite depth values to some extent for\\n\"\n",
    "                      \"  half-floats.\")\n",
    "            sample = sample.to(memory_format=torch.channels_last)\n",
    "            sample = sample.half()\n",
    "\n",
    "        if first_execution or not use_camera:\n",
    "            height, width = sample.shape[2:]\n",
    "            print(f\"    Input resized to {width}x{height} before entering the encoder\")\n",
    "            first_execution = False\n",
    "\n",
    "        prediction = model.forward(sample)\n",
    "        prediction = (\n",
    "            torch.nn.functional.interpolate(\n",
    "                prediction.unsqueeze(1),\n",
    "                size=target_size[::-1],\n",
    "                mode=\"bicubic\",\n",
    "                align_corners=False,\n",
    "            )\n",
    "            .squeeze()\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def create_side_by_side(image, depth, grayscale):\n",
    "    \"\"\"\n",
    "    Take an RGB image and depth map and place them side by side. This includes a proper normalization of the depth map\n",
    "    for better visibility.\n",
    "\n",
    "    Args:\n",
    "        image: the RGB image\n",
    "        depth: the depth map\n",
    "        grayscale: use a grayscale colormap?\n",
    "\n",
    "    Returns:\n",
    "        the image and depth map place side by side\n",
    "    \"\"\"\n",
    "    depth_min = depth.min()\n",
    "    depth_max = depth.max()\n",
    "    normalized_depth = 255 * (depth - depth_min) / (depth_max - depth_min)\n",
    "    normalized_depth *= 3\n",
    "\n",
    "    right_side = np.repeat(np.expand_dims(normalized_depth, 2), 3, axis=2) / 3\n",
    "    if not grayscale:\n",
    "        right_side = cv2.applyColorMap(np.uint8(right_side), cv2.COLORMAP_INFERNO)\n",
    "\n",
    "    if image is None:\n",
    "        return right_side\n",
    "    else:\n",
    "        return np.concatenate((image, right_side), axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55bebe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(dataset, image_column, output_path, model_path, model_type=\"dpt_beit_large_512\", optimize=False, side=False, height=None,\n",
    "        square=False, grayscale=False):\n",
    "    \"\"\"Run MonoDepthNN to compute depth maps.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): path to input folder\n",
    "        output_path (str): path to output folder\n",
    "        model_path (str): path to saved model\n",
    "        model_type (str): the model type\n",
    "        optimize (bool): optimize the model to half-floats on CUDA?\n",
    "        side (bool): RGB and depth side by side in output images?\n",
    "        height (int): inference encoder image height\n",
    "        square (bool): resize to a square resolution?\n",
    "        grayscale (bool): use a grayscale colormap?\n",
    "    \"\"\"\n",
    "    print(\"Initialize\")\n",
    "\n",
    "    # select device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device: %s\" % device)\n",
    "\n",
    "    model, transform, net_w, net_h = load_model(device, model_path, model_type, optimize, height, square)\n",
    "\n",
    "    # get input\n",
    "    num_images = len(dataset)\n",
    "\n",
    "\n",
    "    # create output folder\n",
    "    if output_path is not None:\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    print(\"Start processing\")\n",
    "\n",
    "    if output_path is None:\n",
    "        print(\"Warning: No output path specified. Images will be processed but not shown or stored anywhere.\")\n",
    "    for index, example in enumerate(dataset):\n",
    "        image = example['image']\n",
    "\n",
    "        # input\n",
    "        original_image_rgb = np.array(image)/255.\n",
    "        image = transform({\"image\": original_image_rgb})[\"image\"]\n",
    "\n",
    "        # compute\n",
    "        with torch.no_grad():\n",
    "            prediction = process(device, model, model_type, image, (net_w, net_h), original_image_rgb.shape[1::-1],\n",
    "                                 optimize, False)\n",
    "        # output\n",
    "        if output_path is not None:\n",
    "            filename = os.path.join(\n",
    "                output_path, str(index) + '-' + model_type\n",
    "            )\n",
    "            if not side:\n",
    "                utils.write_depth(filename, prediction, grayscale, bits=2)\n",
    "            else:\n",
    "                original_image_bgr = np.flip(original_image_rgb, 2)\n",
    "                content = create_side_by_side(original_image_bgr*255, prediction, grayscale)\n",
    "                cv2.imwrite(filename + \".png\", content)\n",
    "            utils.write_pfm(filename + \".pfm\", prediction.astype(np.float32))\n",
    "\n",
    "\n",
    "    print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02b94dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_column=\"image\"\n",
    "output_path = \"segmented\"\n",
    "model_path = \"weights/dpt_beit_large_512.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32d88f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize\n",
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\miniconda\\envs\\diffusers\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3191.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded, number of parameters = 345M\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'input_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10652\\3542494970.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_column\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"dpt_beit_large_512\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mside\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msquare\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrayscale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10652\\160661063.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(dataset, image_column, output_path, model_path, model_type, optimize, side, height, square, grayscale)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# get input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0minput_path\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mnum_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_path' is not defined"
     ]
    }
   ],
   "source": [
    "run(dataset, image_column, output_path, model_path, model_type=\"dpt_beit_large_512\", optimize=False, side=False, height=None, square=False, grayscale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afd2ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
